{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Set the root directory to Sorrento Home path\n",
    "root_path = Path.cwd().parents[0]\n",
    "sys.path.append(str(root_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 230773), started 0:00:05 ago. (Use '!kill 230773' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e06c5ddf59b10dbb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e06c5ddf59b10dbb\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ../lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      step           epoch  hp_metric  train_acc  train_fbeta  train_loss  \\\n",
      "0        0             NaN       -1.0        NaN          NaN         NaN   \n",
      "1       49             0.0        NaN    0.65625     0.378788    0.652252   \n",
      "2       99             0.0        NaN    0.68750     0.350877    0.494739   \n",
      "3      105             0.0        NaN        NaN          NaN         NaN   \n",
      "4      149             1.0        NaN    0.81250     0.795455    0.291985   \n",
      "..     ...             ...        ...        ...          ...         ...   \n",
      "766  26349           248.0        NaN    1.00000     1.000000    0.000589   \n",
      "767  26393           248.0        NaN        NaN          NaN         NaN   \n",
      "768  26399           249.0        NaN    1.00000     1.000000    0.001944   \n",
      "769  26449           249.0        NaN    1.00000     1.000000    0.022969   \n",
      "770  26499  [249.0, 249.0]        NaN    1.00000     1.000000    0.000561   \n",
      "\n",
      "      val_acc  val_fbeta  val_loss  \n",
      "0         NaN        NaN       NaN  \n",
      "1         NaN        NaN       NaN  \n",
      "2         NaN        NaN       NaN  \n",
      "3    0.619048   0.290925  0.509296  \n",
      "4         NaN        NaN       NaN  \n",
      "..        ...        ...       ...  \n",
      "766       NaN        NaN       NaN  \n",
      "767  0.994709   0.992438  0.004975  \n",
      "768       NaN        NaN       NaN  \n",
      "769       NaN        NaN       NaN  \n",
      "770  0.994709   0.992438  0.004897  \n",
      "\n",
      "[771 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "from tbparse import SummaryReader\n",
    "log_dir = \"../lightning_logs/JTK_Challenge/version_0/\"\n",
    "reader = SummaryReader(log_dir, pivot=True)\n",
    "df = reader.scalars\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Key val_fbeta was not found in Reservoir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Assuming you have scalar data and you know the tag names, you can convert them to a pandas DataFrame\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Replace 'scalar_tag' with your actual tag name\u001b[39;00m\n\u001b[1;32m     19\u001b[0m scalar_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_fbeta\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# example: 'loss' or 'accuracy'\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m scalar_events \u001b[38;5;241m=\u001b[39m \u001b[43mevent_acc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScalars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscalar_tag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame\u001b[39;00m\n\u001b[1;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([(e\u001b[38;5;241m.\u001b[39mwall_time, e\u001b[38;5;241m.\u001b[39mstep, e\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m scalar_events],\n\u001b[1;32m     24\u001b[0m                   columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwall_time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/github/JTK-Challenge/jtk/lib/python3.11/site-packages/tensorboard/backend/event_processing/event_accumulator.py:603\u001b[0m, in \u001b[0;36mEventAccumulator.Scalars\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mScalars\u001b[39m(\u001b[38;5;28mself\u001b[39m, tag):\n\u001b[1;32m    592\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Given a summary tag, return all associated `ScalarEvent`s.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;124;03m      An array of `ScalarEvent`s.\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mItems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/JTK-Challenge/jtk/lib/python3.11/site-packages/tensorboard/backend/event_processing/reservoir.py:110\u001b[0m, in \u001b[0;36mReservoir.Items\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutex:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buckets:\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m was not found in Reservoir\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key)\n\u001b[1;32m    111\u001b[0m     bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buckets[key]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bucket\u001b[38;5;241m.\u001b[39mItems()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Key val_fbeta was not found in Reservoir'"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your TensorBoard log files\n",
    "#log_dir = 'path_to_your_logs'\n",
    "log_dir = \"../lightning_logs/JTK_Challenge/\"\n",
    "\n",
    "# Initialize an event accumulator\n",
    "event_acc = EventAccumulator(log_dir,\n",
    "                             size_guidance={  # see the tensorboard docs for more details on these parameters\n",
    "                                 'scalars': 0,  # 0 means load all scalar events\n",
    "                             })\n",
    "\n",
    "# Load all events from the log files\n",
    "event_acc.Reload()\n",
    "\n",
    "# Assuming you have scalar data and you know the tag names, you can convert them to a pandas DataFrame\n",
    "# Replace 'scalar_tag' with your actual tag name\n",
    "scalar_tag = 'val_fbeta'  # example: 'loss' or 'accuracy'\n",
    "scalar_events = event_acc.Scalars(scalar_tag)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame([(e.wall_time, e.step, e.value) for e in scalar_events],\n",
    "                  columns=['wall_time', 'step', 'value'])\n",
    "\n",
    "# Optionally, convert wall_time (UNIX timestamp) to a human-readable format\n",
    "df['wall_time'] = pd.to_datetime(df['wall_time'], unit='s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Main directory containing all runs\n",
    "main_log_dir = \"../lightning_logs/JTK_Challenge/\"\n",
    "\n",
    "# Initialize an empty DataFrame to hold all data\n",
    "all_runs_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each subdirectory (each run)\n",
    "for i, run_dir in enumerate(os.listdir(main_log_dir)):\n",
    "    run_path = os.path.join(main_log_dir, run_dir)\n",
    "    if os.path.isdir(run_path):  # Check if it's a directory\n",
    "        # Initialize an event accumulator for the current run\n",
    "        event_acc = EventAccumulator(run_path, size_guidance={'scalars': 0})\n",
    "        event_acc.Reload()\n",
    "\n",
    "        # Assuming you are interested in scalar data\n",
    "        scalar_tag = 'val_fbeta'  # Replace with your actual tag\n",
    "        if scalar_tag in event_acc.Tags()['scalars']:  # Check if the scalar tag exists\n",
    "            scalar_events = event_acc.Scalars(scalar_tag)\n",
    "\n",
    "            # Convert to DataFrame for the current run\n",
    "            run_df = pd.DataFrame([(e.wall_time, e.step, e.value) for e in scalar_events],\n",
    "                                  columns=['wall_time', 'step', 'value'])\n",
    "\n",
    "            # Add run identifier: here we use run_dir as identifier, but you could also use i (the run number)\n",
    "            run_df['run_id'] = run_dir  # or use `i` for a numerical identifier\n",
    "\n",
    "            # Optionally, convert wall_time to a readable format\n",
    "            run_df['wall_time'] = pd.to_datetime(run_df['wall_time'], unit='s')\n",
    "\n",
    "            # Append the current run's data to the all_runs_df DataFrame\n",
    "            all_runs_df = pd.concat([all_runs_df, run_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        wall_time   step     value      run_id\n",
      "0   2024-03-30 02:17:29.113399744     52  0.034853  version_91\n",
      "1   2024-03-30 02:17:29.885790825    105  0.797964  version_91\n",
      "2   2024-03-30 02:17:30.362987757    158  0.122309  version_91\n",
      "3   2024-03-30 02:17:30.881132364    211  0.045010  version_91\n",
      "4   2024-03-30 02:17:31.354292631    264  0.868784  version_91\n",
      "..                            ...    ...       ...         ...\n",
      "245 2024-03-30 02:19:31.351519346  13037  1.000000  version_91\n",
      "246 2024-03-30 02:19:31.864068747  13090  0.992860  version_91\n",
      "247 2024-03-30 02:19:32.359914541  13143  0.992860  version_91\n",
      "248 2024-03-30 02:19:32.878743887  13196  0.992860  version_91\n",
      "249 2024-03-30 02:19:33.377018690  13249  0.992860  version_91\n",
      "\n",
      "[250 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(all_runs_df.head(250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reader.scalars.pivot(index=\"epoch\", columns=\"tag\", values=\"value\")\n",
    "output = {}\n",
    "for tag in reader.scalars[\"tag\"].unique():\n",
    "    output[tag] = reader.scalars[reader.scalars[\"tag\"] == tag][\"value\"].values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "epoch          780\n",
       "train_acc      530\n",
       "train_fbeta    530\n",
       "train_loss     530\n",
       "val_acc        250\n",
       "val_fbeta      250\n",
       "val_loss       250\n",
       "hp_metric        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.scalars[\"tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/JTK-Challenge/jtk/lib/python3.11/site-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/github/JTK-Challenge/jtk/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/JTK-Challenge/jtk/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/github/JTK-Challenge/jtk/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
